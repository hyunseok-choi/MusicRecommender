{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "1. Ratings matrix\n",
    "2. Item Feature matrix\n",
    "3. User Feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30755x359966 sparse matrix of type '<class 'numpy.int8'>'\n",
       "\twith 7377418 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the main ratings matrix\n",
    "f = open(\"data/X_mat_jupyter.data\", 'rb')\n",
    "X_mat = pickle.load(f)\n",
    "f.close() # save the sparse matrix to a file\n",
    "\n",
    "X_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load feature matrices\n",
    "X_item_features = pd.read_csv('data/X_item_features.csv')\n",
    "X_user_features = pd.read_csv('data/X_user_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install LightFM if you didn't install it.\n",
    "# !pip install LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import recall_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "from lightfm.evaluation import reciprocal_rank\n",
    "import lightfm.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(tr, ts) = lightfm.cross_validation.random_train_test_split(X_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Populate Feature Matrices\n",
    "item_features_col_names = X_item_features.columns.values.tolist()\n",
    "user_features_col_names = X_user_features.columns.values.tolist()\n",
    "item_features_col_names.remove('item_id')\n",
    "item_features_col_names.remove('Unnamed: 0')\n",
    "user_features_col_names.remove('user_id')\n",
    "user_features_col_names.remove('Unnamed: 0')\n",
    "item_features = csr_matrix(X_item_features[item_features_col_names])\n",
    "user_features = csr_matrix(X_user_features[user_features_col_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359966, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_item_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30755, 42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_user_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(item_features_col_names))\n",
    "print(len(user_features_col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of threads; you can increase this\n",
    "# if you have more physical cores available.\n",
    "NUM_THREADS = 8\n",
    "NUM_COMPONENTS = 79\n",
    "NUM_EPOCHS = 3 \n",
    "ITEM_ALPHA = 1e-3 # L2 penalty on item features\n",
    "USER_ALPHA = 1e-3 # L2 penalty on user features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new model instance\n",
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                user_alpha=USER_ALPHA,\n",
    "                no_components=NUM_COMPONENTS,\n",
    "                random_state=0)\n",
    "\n",
    "# Fit the hybrid model. Note that this time, we pass\n",
    "# in the item features matrix.\n",
    "model = model.fit(tr,\n",
    "                  item_features=item_features,\n",
    "                  user_features=user_features,\n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  num_threads=NUM_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recommendation System Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid test set AUC: 0.83231\n"
     ]
    }
   ],
   "source": [
    "# Test score\n",
    "test_auc = auc_score(model,\n",
    "                    ts,\n",
    "                    train_interactions=tr,\n",
    "                    item_features=item_features,\n",
    "                     user_features=user_features,\n",
    "                    num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test set AUC: %s' % test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Evaluation Metrics\n",
    "### Set the median of the recommendation as a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, recall_score, precision_score, f1_score,  roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_rank(ts, train_interactions=tr, item_features=item_features,\n",
    "                  user_features=user_features, num_threads=NUM_THREADS)\n",
    "\n",
    "# import pickle\n",
    "# f = open(\"test_pred.data\", 'wb')\n",
    "# pickle.dump(pred, f)\n",
    "# f.close() # save the sparse matrix to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.545850717459\n",
      "Precision:  0.5487558898\n",
      "Recall:  0.545568484123\n",
      "F1:  0.547157545031\n",
      "AUC:  0.545852365935\n"
     ]
    }
   ],
   "source": [
    "threshold = np.median(test_pred.data)\n",
    "print(\"Accuracy: \", accuracy_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Precision: \", precision_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Recall: \", recall_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"F1: \", f1_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "fpr, tpr, _ = roc_curve(ts.tocsr().data>1, test_pred.data<threshold)\n",
    "print(\"AUC: \", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.544466764804\n",
      "Precision:  0.552284873039\n",
      "Recall:  0.497482567996\n",
      "F1:  0.523453264342\n",
      "AUC:  0.544741191287\n"
     ]
    }
   ],
   "source": [
    "threshold = np.median(test_pred.data)*0.8\n",
    "print(\"Accuracy: \", accuracy_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Precision: \", precision_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Recall: \", recall_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"F1: \", f1_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "fpr, tpr, _ = roc_curve(ts.tocsr().data>1, test_pred.data<threshold)\n",
    "print(\"AUC: \", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.545386463018\n",
      "Precision:  0.550508337126\n",
      "Recall:  0.52329028902\n",
      "F1:  0.536554359128\n",
      "AUC:  0.545515522909\n"
     ]
    }
   ],
   "source": [
    "threshold = np.median(test_pred.data)*0.9\n",
    "print(\"Accuracy: \", accuracy_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Precision: \", precision_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Recall: \", recall_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"F1: \", f1_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "fpr, tpr, _ = roc_curve(ts.tocsr().data>1, test_pred.data<threshold)\n",
    "print(\"AUC: \", auc(fpr, tpr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.546165190541\n",
      "Precision:  0.545913798745\n",
      "Recall:  0.580056493977\n",
      "F1:  0.562467493433\n",
      "AUC:  0.54596723736\n"
     ]
    }
   ],
   "source": [
    "threshold = np.median(test_pred.data)*1.2\n",
    "print(\"Accuracy: \", accuracy_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Precision: \", precision_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Recall: \", recall_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"F1: \", f1_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "fpr, tpr, _ = roc_curve(ts.tocsr().data>1, test_pred.data<threshold)\n",
    "print(\"AUC: \", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.54585817264\n",
      "Precision:  0.543980878676\n",
      "Recall:  0.599629932105\n",
      "F1:  0.570451440773\n",
      "AUC:  0.54554410119\n"
     ]
    }
   ],
   "source": [
    "threshold = np.median(test_pred.data)*1.4\n",
    "print(\"Accuracy: \", accuracy_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Precision: \", precision_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"Recall: \", recall_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "print(\"F1: \", f1_score(ts.tocsr().data>1, test_pred.data<threshold))\n",
    "fpr, tpr, _ = roc_curve(ts.tocsr().data>1, test_pred.data<threshold)\n",
    "print(\"AUC: \", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
